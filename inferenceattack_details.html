<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NextBuys.co | Project Details</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
  <script src="https://unpkg.com/lucide@latest"></script>
  <style>
    body {
      font-family: 'Inter', sans-serif;
      line-height: 1.75;
    }
    .project-section img {
      border-radius: 8px;
      margin: 1rem 0;
    }
    .highlight {
      background-color: #f3f4f6;
      padding: 0.5rem 1rem;
      border-left: 4px solid #3b82f6;
      margin: 1rem 0;
    }
    a {
      color: #3b82f6;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>

<body class="bg-white">
  <div class="flex h-screen">
    <!-- Sidebar -->
    <aside class="w-64 bg-gray-100 shadow-lg p-6 flex flex-col justify-between">
      <div>
        <div class="flex items-center space-x-3 mb-10">
          <div class="w-12 h-12 bg-gray-300 rounded-full"> <img src="./assets/profilepic.jpeg" alt="PGW Logo"></div>
          <div>
            <h1 class="text-lg font-bold">Gaurav Shetty</h1>
            <p class="text-sm text-gray-600">Masters in Data Science</p>
          </div>
        </div>

        <nav class="space-y-2">
          <a href="index.html" class="nav-item flex items-center space-x-3 p-2 rounded-lg cursor-pointer">
            <i data-lucide="home" class="text-gray-600"></i>
            <span class="text-gray-800 font-medium">Home</span>
          </a>
          <a href="projects.html" class="nav-item flex items-center space-x-3 p-2 rounded-lg cursor-pointer">
            <i data-lucide="folder" class="text-gray-600"></i>
            <span class="text-gray-800 font-medium">Projects</span>
          </a>
          <a href="experience.html" class="nav-item flex items-center space-x-3 p-2 rounded-lg cursor-pointer">
            <i data-lucide="briefcase" class="text-gray-600"></i>
            <span class="text-gray-800 font-medium">Experience</span>
          </a>
          <a href="articles.html" class="nav-item flex items-center space-x-3 p-2 rounded-lg cursor-pointer">
            <i data-lucide="file-text" class="text-gray-600"></i>
            <span class="text-gray-800 font-medium">Contact</span>
          </a>
        </nav>
      </div>

      
      <div class="space-y-2">
        <h2 class="text-sm text-gray-600">Socials</h2>
        <a href="https://linkedin.com" target="_blank" class="nav-item p-2 rounded-lg cursor-pointer">LinkedIn</a>
        <a href="https://github.com" target="_blank" class="nav-item p-2 rounded-lg cursor-pointer">Github</a>
      </div>
    </aside>

    <!-- Main Content -->
    <main id="main-content" class="flex-1 p-10 overflow-auto flex justify-center bg-white">
      <div class="max-w-4xl text-left project-section">
        <h1 class="text-3xl font-bold mb-4">A Study on Adversarial Attacks and Defenses in Deep Learning Models</h1>
        <p class="text-gray-600 mb-6">This project explores inference attacks on machine learning models, highlighting both attack strategies and defense mechanisms. 
          As machine learning becomes increasingly prevalent, it is crucial to recognize that these systems are also vulnerable to adversarial attacks. 
          Using a PyTorch-based computer vision model for vehicle damage assessment, we demonstrate how an attacker can exploit weaknesses using the Fast Gradient Sign Method (FGSM) to significantly degrade model accuracy. 
          Additionally, we showcase defensive techniques such as Gaussian Noise Augmentation and adversarial training to mitigate these attacks and restore model performance. This project underscores the importance of robust AI security in real-world applications.</p>

        <img src="./assets/nextbuys_overview.jpeg" alt="NextBuys Overview" class="w-full h-auto">

        <h2 class="text-2xl font-semibold mt-8 mb-2">Objective</h2>
        <p class="text-gray-600 mb-6">This project aims to analyze the impact of adversarial attacks on deep learning models, focusing on inference attacks and defense mechanisms in computer vision applications.  
          The objective is to highlight the vulnerabilities of machine learning models to adversarial manipulation and evaluate countermeasures to ensure secure and reliable AI systems in real-world applications</p>

        <div class="highlight">
          <strong>Key Focus:</strong> Observing how vulnerable are the machine learning models against these attacks and the effectiveness of defense mechanisms.
        </div>

        <h2 class="text-2xl font-semibold mt-8 mb-2">Methodology</h2>
        <p class="text-gray-600 mb-4">The project follows a structured approach, beginning with the development and training of a computer vision model for vehicle damage assessment using PyTorch. 
          After establishing a baseline accuracy of 77%, we implement FGSM attacks to craft adversarial samples and assess their impact on model performance. 
          The model performance dropped to 16% based on these attacks. 
          We also look into defensive techniques such as Gaussian Noise Augmentation, introducing controlled noise during training, and implement adversarial training, which retrains the model with adversarial examples to improve its resistance. </p>

        <img src="./assets/nextbuys_methodology.jpeg" alt="Methodology Diagram" class="w-full h-auto">

        <p class="text-gray-600 mb-6">The effectiveness of these defenses is measured by evaluating key performance metrics, including accuracy degradation, robustness improvements, and adversarial resistance. 
          Visualization techniques such as confusion matrices and perturbation heatmaps further illustrate the attack and defense dynamics.</p>

        <h2 class="text-2xl font-semibold mt-8 mb-2">Tools & Technologies</h2>
        <ul class="list-disc list-inside text-gray-600 mb-6">
          <li>Utilized the ART library in python to program the attacks. </li>
          <li>PyTorch and Torchvision to build the convolutional neural network</li>
          <li>LIME for model explainability.</li>
        </ul>

        <h2 class="text-2xl font-semibold mt-8 mb-2">Outcome</h2>
        <p class="text-gray-600 mb-6">This study demonstrates how even machine learning models are exposed to security risks like these attacks which can degrade the model performance. 
          It also shows ways to defend against these attack emphasizing the necessity of adversarial defense mechanisms in securing AI applications</p>

        <img src="./assets/nextbuys_result.jpeg" alt="Project Outcome" class="w-full h-auto">

        <div class="flex space-x-4 mt-8">
          <a href="https://github.com/gauravshetty/nextbuys" target="_blank" class="px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600">View on GitHub</a>
          <a href="https://nextbuys.co" target="_blank" class="px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600">Visit Website</a>
        </div>
      </div>
    </main>
  </div>

  <script>
    lucide.createIcons();
  </script>
</body>

</html>
